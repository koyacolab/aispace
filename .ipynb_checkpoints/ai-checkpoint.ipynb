{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1698346597091,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "Qj506dTf0hD4"
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18320,
     "status": "ok",
     "timestamp": 1698346615405,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "tkYBYiue0lS7",
    "outputId": "fc36a567-f523-4e53-9362-e88287f10382"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')\n",
    "\n",
    "# import os\n",
    "\n",
    "# # !pip install fire\n",
    "# # !pip install tqdm\n",
    "\n",
    "# home_dir = '/content/gdrive/My Drive/A0/aispace'\n",
    "# os.chdir(home_dir)\n",
    "# !pwd\n",
    "\n",
    "# import os\n",
    "# # Get the current working directory\n",
    "# current_directory = os.getcwd()\n",
    "# print(current_directory)\n",
    "\n",
    "# import shutil\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46999,
     "status": "ok",
     "timestamp": 1698346662401,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "pEOnhw3U0hD6",
    "outputId": "729c0b36-a21c-4a7c-b2a7-780072db6e4a"
   },
   "outputs": [],
   "source": [
    "# !pip install rasterio\n",
    "# !pip install accelerate\n",
    "# !pip install peft\n",
    "# !pip install transformers\n",
    "# # !pip install transformers==4.33.0\n",
    "# !pip install datasets\n",
    "# # !python3 -m pip install --upgrade pip\n",
    "# # !pip install Sophia-Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1698346662402,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "3_zkf2dM0hD6"
   },
   "outputs": [],
   "source": [
    "KAGGLE = False\n",
    "if KAGGLE == True:\n",
    "    # Define the input and output directories\n",
    "    input_directory  = '/kaggle/input/aidataset'  # Replace with the path to your input directory\n",
    "    output_directory = '/kaggle/working'  # Replace with the path to your output directory\n",
    "\n",
    "    def input_copy(input_directory, output_directory):\n",
    "        # Get a list of files in the input directory\n",
    "        files_to_copy = os.listdir(input_directory)\n",
    "        # Iterate through the files and copy them to the output directory\n",
    "        for file_name in files_to_copy:\n",
    "            # Create the full paths for the source and destination\n",
    "            source_file = os.path.join(input_directory, file_name)\n",
    "            destination_file = os.path.join(output_directory, file_name)\n",
    "\n",
    "            # Copy the file from the source to the destination\n",
    "            shutil.copy(source_file, destination_file)\n",
    "\n",
    "        # Get a list of files in the input directory\n",
    "        files = os.listdir(output_directory)\n",
    "        print(files)\n",
    "\n",
    "    input_copy(input_directory, output_directory)\n",
    "\n",
    "    source_directory  = '/kaggle/input/'  # Replace with the path to your input directory\n",
    "    destination_directory = '/kaggle/working/input'  # Replace with the path to your output directory\n",
    "\n",
    "    # Copy the source directory to the destination directory\n",
    "    shutil.copytree(source_directory, destination_directory)\n",
    "\n",
    "    input_copy(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1698346662402,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "5fzTgKJLQtKK"
   },
   "outputs": [],
   "source": [
    "# import sophia\n",
    "\n",
    "# !pip install wandb\n",
    "\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17558,
     "status": "ok",
     "timestamp": 1698346679956,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "REzgouGF0hD7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import great\n",
    "from great import GReaT\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "################################\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# all imports should go here\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import skimage.exposure\n",
    "\n",
    "# access package for AWS access\n",
    "# import boto3\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import datetime\n",
    "import platform\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import ee\n",
    "import h5py\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta  # Import timedelta here\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import rasterio as rio\n",
    "################################\n",
    "\n",
    "\n",
    "\n",
    "from hlsdataset import HLSDataSet\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Execute only once!\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6052,
     "status": "ok",
     "timestamp": 1698346685999,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "XtgFFiGF0hD7",
    "outputId": "42406310-0b1a-450f-e32d-77dae7933fd1"
   },
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "\n",
    "print(f'NumPy version:{np.__version__}')\n",
    "np.float = float # 'float32' # float\n",
    "table_dtype = np.float  #'float32'\n",
    "print(table_dtype)\n",
    "\n",
    "hls_data = HLSDataSet(table_dtype = table_dtype)\n",
    "\n",
    "hls_data.clip_dataset(x1=50.0, y1=50.0, x2=100.0, y2=100.0)\n",
    "\n",
    "doys = [171, 179, 187, 195, 203, 211, 219]\n",
    "doys = [203, 211, 219]\n",
    "doys = [203, 211, 219]\n",
    "df = hls_data._get_data_doys(doys = doys, SHOW=True)\n",
    "display(df)\n",
    "\n",
    "# df = hls_data._set_columns_name()\n",
    "# display(df)\n",
    "\n",
    "df1, df2 = hls_data._nan_9999()\n",
    "# display(df1)\n",
    "# display(df2)\n",
    "\n",
    "df1, df2 = hls_data._set_clear_cloud()\n",
    "# display(df1)\n",
    "# display(df2)\n",
    "\n",
    "data, nan, clear, cloud = hls_data._set_train_columns_name()\n",
    "\n",
    "print('clear')\n",
    "display(clear)\n",
    "\n",
    "# train_data, test_data = hls_data._set_train_test_data(doy=211.0, x1=60.0, y1=40.0, x2=75.0, y2=55.0)\n",
    "\n",
    "train_data, test_data = hls_data._set_train_test_data(doy=211.0, x1=45.0, y1=45.0, x2=50.0, y2=50.0)\n",
    "# train_data, test_data = hls_data._set_timeseries_train_test_data(doy=211.0, x1=50.0, y1=50.0, x2=100.0, y2=100.0)\n",
    "\n",
    "print('train_data')\n",
    "display(train_data)\n",
    "print('test_data')\n",
    "display(test_data)\n",
    "# display(data)\n",
    "# display(nan)\n",
    "# display(clear)\n",
    "# display(cloud)\n",
    "\n",
    "hls_data._to_impute()\n",
    "hls_data._inference_train_test_data()\n",
    "hls_data._inference_imshow()\n",
    "\n",
    "# fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE DATA 4 TRAIN #####################################\n",
    "train_columns_list = ['B02', 'B03', 'B04', 'PID', 'DOY']\n",
    "# train_columns_list = ['B03', 'PID', 'DOY']\n",
    "data = train_data[train_columns_list].copy()\n",
    "\n",
    "display(data)\n",
    "\n",
    "# Create a new DataFrame with four columns\n",
    "data = data.melt(id_vars=['PID', 'DOY'], value_vars=['B02', 'B03', 'B04'],\n",
    "                    var_name='BNAME', value_name='BAND')\n",
    "\n",
    "data['BNAME'] = data['BNAME'].astype(str)\n",
    "#############################################################\n",
    "\n",
    "# Get only trainable columns ###########################\n",
    "train_columns_list = ['DOY', 'PID', 'BNAME', 'BAND']\n",
    "\n",
    "data = data[train_columns_list].copy()\n",
    "\n",
    "# data = data.astype(table_dtype)\n",
    "print(data.dtypes.tolist())\n",
    "# Reset the index to remove it\n",
    "data = data.reset_index(drop=True)\n",
    "# train_data.columns = final_columns_list\n",
    "print(f'Code data 4 trainable is for {train_columns_list}')\n",
    "display(data)\n",
    "\n",
    "print('Ready for train...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### TRAIN TOKENIZER FOR HLS DATA ################################################\n",
    "# #### TRAIN ONCE AND COMMENT FOR AVOID FORKING TOKENIZER ##########################\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()\n",
    "\n",
    "ReTRAIN_TOKENIZER = False\n",
    "if ReTRAIN_TOKENIZER == True:\n",
    "    def _get_synth_data():\n",
    "        BAND = [b for b in range(0, 10)]\n",
    "        PID  = [p for p in range(0, 10)]\n",
    "        DOY  = [d for d in range(0, 10)]   \n",
    "        # BNAME = ['B02', 'B03', 'B04']\n",
    "        # Create a DataFrame with specified column names\n",
    "        synth_data = pd.DataFrame(list(zip(BAND, PID, DOY)), columns=['BAND', 'PID', 'DOY'])\n",
    "\n",
    "        # # Create a list of tuples with all possible combinations of DOY and PID\n",
    "        # synth_data = [(d, p, b) for d in DOY for p in PID for b in BAND]\n",
    "        # # Create a DataFrame from the data\n",
    "        # synth_data = pd.DataFrame(synth_data, columns=[\"DOY\", \"PID\", \"B03\"])\n",
    "        return synth_data.astype(table_dtype)\n",
    "    \n",
    "    synth_data = _get_synth_data()\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    display(synth_data)\n",
    "    \n",
    "    from transformers import AutoTokenizer\n",
    "    from aitokenizer import TrainTokenizer, TrainCorpus, PreTrainTokenizer\n",
    "    \n",
    "    old_tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "    old_tokenizer.pad_token = old_tokenizer.eos_token\n",
    "    \n",
    "    data_corpus = TrainCorpus(data)._get_training_corpus()\n",
    "\n",
    "    sample = next(data_corpus)[0]\n",
    "    print('L8 data:', sample)\n",
    "    \n",
    "    tokens = old_tokenizer.tokenize(sample)\n",
    "    print('old_tokenizer:', tokens)\n",
    "    # print(old_tokenizer(sample).tokens())\n",
    "\n",
    "    digits_tok = [str(x) for x in range(0,10)]\n",
    "    spec_tok = ['B02', 'B03', 'B04']\n",
    "    # print()\n",
    "    # new_tokenizer = TrainTokenizer(synth_data, sample, old_tokenizer)._train(special_tokens=train_columns_list)\n",
    "    new_tokenizer = PreTrainTokenizer(data, old_tokenizer)._train(special_tokens=train_columns_list+spec_tok)   #+digits_tok)\n",
    "\n",
    "    # print(new_tokenizer(sample).tokens())\n",
    "    tokens = new_tokenizer.tokenize(sample)\n",
    "    print('aispace-tokenizer:', tokens)\n",
    "    \n",
    "    new_tokenizer.save_pretrained(\"aispace-tokenizer\")\n",
    "\n",
    "    print('Tokenizer trained, COMMENT & RELOAD Tokenizer Trainer...')\n",
    "\n",
    "    fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AHzEWLYbB-IY"
   },
   "outputs": [],
   "source": [
    "# Split the DataFrame into training and test DataFrames\n",
    "# train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "# train_data = train_data.reset_index(drop=True)\n",
    "# display(train_data)\n",
    "# test_data = test_data.reset_index(drop=True)\n",
    "# display(test_data)\n",
    "\n",
    "EXP_NAME = 'exp-A100'\n",
    "EXP_NAME = 'exp-V100'\n",
    "EXP_NAME = 'exp-T4'\n",
    "\n",
    "\n",
    "EXP_NAME = f'A0[ONE_TOK[ai]]'\n",
    "EXP_NAME = f'A0[ONE_TOK[gpt]]'\n",
    "\n",
    "# #### FOR V-100 GPU USABILITY################################\n",
    "# epochs_steps = 2000   # 250 #50\n",
    "# save_steps = 1000   #250 #50 #9380  #2000   # FOR FIT THIS VALUE START TRAIN DATASET, EQUALS FOR STEPS IN FIRST START\n",
    "# logging_steps = 200 # 250 #25 #9380  #1000\n",
    "# #### batch_size = 250 for T4\n",
    "# batch_size = 200 #RGBN\n",
    "# batch_size = 250 # 5 columns trainable \n",
    "# batch_size = 250 #480 # 3 columns trainable \n",
    "# #######################################\n",
    "step_checkpoint_list = [7000, 105000, 82000, 32000, 22000, 112000, 112000, 112000]\n",
    "step_checkpoint_list = [112000, 32000, 21000]\n",
    "step_checkpoint_list = [5000, ]\n",
    "\n",
    "\n",
    "TRAINER_RUN = 0\n",
    "RESUME_FROM_CHECKPOINT = False\n",
    "\n",
    "if TRAINER_RUN == 0:\n",
    "    load_model_path = 'distilgpt2'\n",
    "    experiment_dir = f\"{EXP_NAME}/run[{TRAINER_RUN}]\"\n",
    "elif TRAINER_RUN > 0:\n",
    "    # before = pd.read_csv(f'{EXP_NAME}/run[{TRAINER_RUN-1}].csv')\n",
    "    # load_model_path = before['experiment_dir'][0] + f'/checkpoint-{step_checkpoint}'\n",
    "    if RESUME_FROM_CHECKPOINT == True:\n",
    "        step_checkpoint = step_checkpoint_list[TRAINER_RUN]\n",
    "        load_model_path = f'{EXP_NAME}/run[{TRAINER_RUN}]' + f'/checkpoint-{step_checkpoint}'\n",
    "        print(f'Resume from checkpoint:{load_model_path}')\n",
    "    else:\n",
    "        step_checkpoint = step_checkpoint_list[TRAINER_RUN-1]\n",
    "        load_model_path = f'{EXP_NAME}/run[{TRAINER_RUN-1}]' + f'/checkpoint-{step_checkpoint}'\n",
    "    experiment_dir  = f\"{EXP_NAME}/run[{TRAINER_RUN}]\"\n",
    "\n",
    "#### TRAINER HYPERPARAMETERS #############################\n",
    "save_steps = 1000\n",
    "logging_steps = 200\n",
    "\n",
    "epochs = 2000\n",
    "batch_size = 350\n",
    "\n",
    "learning_rate = 1e-6\n",
    "lr_scheduler_type = 'constant'\n",
    "# lr_scheduler_type = 'cosine'\n",
    "num_cycles = 4\n",
    "\n",
    "warmup_steps = 15000\n",
    "\n",
    "optimizer = 'sophia'\n",
    "##########################################################\n",
    "\n",
    "TRAINER_DICT = pd.DataFrame({'EXP_NAME' : EXP_NAME}, index = [TRAINER_RUN])    # dict()\n",
    "TRAINER_DICT['TRAINER_RUN'] = TRAINER_RUN\n",
    "TRAINER_DICT['table_dtype'] = table_dtype\n",
    "TRAINER_DICT['experiment_dir'] = experiment_dir\n",
    "TRAINER_DICT['save_steps'] = save_steps\n",
    "TRAINER_DICT['logging_steps'] = logging_steps\n",
    "TRAINER_DICT['epochs'] = epochs\n",
    "TRAINER_DICT['batch_size'] = batch_size\n",
    "TRAINER_DICT['learning_rate'] = learning_rate\n",
    "TRAINER_DICT['lr_scheduler_type'] = lr_scheduler_type\n",
    "TRAINER_DICT['warmup_steps'] = warmup_steps\n",
    "TRAINER_DICT['num_cycles'] = num_cycles\n",
    "TRAINER_DICT['optimizer'] = optimizer\n",
    "TRAINER_DICT['fp16'] = True\n",
    "\n",
    "display(pd.DataFrame.from_dict(TRAINER_DICT))\n",
    "\n",
    "##### SET experiment_dir & efficient_finetuning #######################\n",
    "efficient_finetuning = ''  #'lora'\n",
    "if efficient_finetuning == 'lora':\n",
    "    experiment_dir = f\"{experiment_dir}_{efficient_finetuning}\"\n",
    "\n",
    "\n",
    "print('experiment_dir :', experiment_dir)\n",
    "print('load_model_path:', load_model_path)\n",
    "\n",
    "# Get the current CPU time in seconds since the epoch\n",
    "current_time = int(time.time())\n",
    "# Use the current time as a seed for a random number generator\n",
    "random_seed_state = current_time  # You can use this random_state for various random processes\n",
    "\n",
    "model = GReaT(llm=load_model_path,\n",
    "              # tokenizer=load_model_path,\n",
    "              tokenizer='aispace-tokenizer',\n",
    "              batch_size=batch_size, epochs=epochs, \n",
    "              logging_steps=logging_steps, save_steps=save_steps,\n",
    "              # evaluation_strategy='steps',\n",
    "              # dataloader_num_workers=2, #fp16=True,\n",
    "              logging_first_step=True,\n",
    "              save_total_limit=2,\n",
    "              prediction_loss_only=True,\n",
    "              experiment_dir=experiment_dir,\n",
    "              dataloader_num_workers=2,\n",
    "              efficient_finetuning = efficient_finetuning,\n",
    "              learning_rate=learning_rate,\n",
    "              lr_scheduler_type=lr_scheduler_type,\n",
    "              warmup_steps = warmup_steps,\n",
    "              num_cycles = num_cycles,\n",
    "              # warmup_ratio=0.05,\n",
    "              seed=current_time,\n",
    "              data_seed=current_time+int(time.time()),\n",
    "              # optim=TRAINER_DICT['optimizer'],\n",
    "              fp16 = True,\n",
    "              # torch_compile=True,   #### uncomment for Ampere\n",
    "              # bf16=True,            #### uncomment for Ampere\n",
    "              )\n",
    "\n",
    "print(f'----------- Model architecture, efficient_finetuning: {efficient_finetuning} -----------------------')\n",
    "print(model.model)\n",
    "print(f'----------------------------------------------------------------------------------------------------')\n",
    "# fn\n",
    "\n",
    "if RESUME_FROM_CHECKPOINT == True:\n",
    "    model.fit(data=data, resume_from_checkpoint=True)\n",
    "else:\n",
    "    model.fit(data=data)\n",
    "# model.fit(data=train_data, test_data=test_data)\n",
    "\n",
    "model.save(f'{experiment_dir}/model')\n",
    "\n",
    "TRAINER_DICT.to_csv(f'{experiment_dir}.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LV037qqBNdol"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "695cBCyoBG3q"
   },
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9v8lVF5Sayy"
   },
   "outputs": [],
   "source": [
    "fn"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
