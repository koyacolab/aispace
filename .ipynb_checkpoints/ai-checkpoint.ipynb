{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1698346597091,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "Qj506dTf0hD4"
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18320,
     "status": "ok",
     "timestamp": 1698346615405,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "tkYBYiue0lS7",
    "outputId": "fc36a567-f523-4e53-9362-e88287f10382"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')\n",
    "\n",
    "# import os\n",
    "\n",
    "# # !pip install fire\n",
    "# # !pip install tqdm\n",
    "\n",
    "# home_dir = '/content/gdrive/My Drive/A0/aispace'\n",
    "# os.chdir(home_dir)\n",
    "# !pwd\n",
    "\n",
    "# import os\n",
    "# # Get the current working directory\n",
    "# current_directory = os.getcwd()\n",
    "# print(current_directory)\n",
    "\n",
    "# import shutil\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46999,
     "status": "ok",
     "timestamp": 1698346662401,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "pEOnhw3U0hD6",
    "outputId": "729c0b36-a21c-4a7c-b2a7-780072db6e4a"
   },
   "outputs": [],
   "source": [
    "# !pip install rasterio\n",
    "# !pip install accelerate\n",
    "# !pip install peft\n",
    "# !pip install transformers\n",
    "# # !pip install transformers==4.33.0\n",
    "# !pip install datasets\n",
    "# # !python3 -m pip install --upgrade pip\n",
    "# # !pip install Sophia-Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1698346662402,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "3_zkf2dM0hD6"
   },
   "outputs": [],
   "source": [
    "KAGGLE = False\n",
    "if KAGGLE == True:\n",
    "    # Define the input and output directories\n",
    "    input_directory  = '/kaggle/input/aidataset'  # Replace with the path to your input directory\n",
    "    output_directory = '/kaggle/working'  # Replace with the path to your output directory\n",
    "\n",
    "    def input_copy(input_directory, output_directory):\n",
    "        # Get a list of files in the input directory\n",
    "        files_to_copy = os.listdir(input_directory)\n",
    "        # Iterate through the files and copy them to the output directory\n",
    "        for file_name in files_to_copy:\n",
    "            # Create the full paths for the source and destination\n",
    "            source_file = os.path.join(input_directory, file_name)\n",
    "            destination_file = os.path.join(output_directory, file_name)\n",
    "\n",
    "            # Copy the file from the source to the destination\n",
    "            shutil.copy(source_file, destination_file)\n",
    "\n",
    "        # Get a list of files in the input directory\n",
    "        files = os.listdir(output_directory)\n",
    "        print(files)\n",
    "\n",
    "    input_copy(input_directory, output_directory)\n",
    "\n",
    "    source_directory  = '/kaggle/input/'  # Replace with the path to your input directory\n",
    "    destination_directory = '/kaggle/working/input'  # Replace with the path to your output directory\n",
    "\n",
    "    # Copy the source directory to the destination directory\n",
    "    shutil.copytree(source_directory, destination_directory)\n",
    "\n",
    "    input_copy(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1698346662402,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "5fzTgKJLQtKK"
   },
   "outputs": [],
   "source": [
    "# import sophia\n",
    "\n",
    "# !pip install wandb\n",
    "\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17558,
     "status": "ok",
     "timestamp": 1698346679956,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "REzgouGF0hD7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import great\n",
    "from great import GReaT\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "################################\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# all imports should go here\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import skimage.exposure\n",
    "\n",
    "# access package for AWS access\n",
    "# import boto3\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import datetime\n",
    "import platform\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import ee\n",
    "import h5py\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta  # Import timedelta here\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import rasterio as rio\n",
    "################################\n",
    "\n",
    "\n",
    "\n",
    "from hlsdataset import HLSDataSet\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Execute only once!\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6052,
     "status": "ok",
     "timestamp": 1698346685999,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "XtgFFiGF0hD7",
    "outputId": "42406310-0b1a-450f-e32d-77dae7933fd1"
   },
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "\n",
    "print(f'NumPy version:{np.__version__}')\n",
    "np.float = float # 'float32' # float\n",
    "table_dtype = np.float  #'float32'\n",
    "print(table_dtype)\n",
    "\n",
    "hls_data = HLSDataSet(table_dtype = table_dtype)\n",
    "\n",
    "hls_data.clip_dataset(x1=50.0, y1=50.0, x2=100.0, y2=100.0)\n",
    "\n",
    "doys = [171, 179, 187, 195, 203, 211, 219]\n",
    "doys = [203, 211, 219]\n",
    "df = hls_data._get_data_doys(doys = doys, SHOW=True)\n",
    "display(df)\n",
    "\n",
    "# df = hls_data._set_columns_name()\n",
    "# display(df)\n",
    "\n",
    "df1, df2 = hls_data._nan_9999()\n",
    "# display(df1)\n",
    "# display(df2)\n",
    "\n",
    "df1, df2 = hls_data._set_clear_cloud()\n",
    "# display(df1)\n",
    "# display(df2)\n",
    "\n",
    "data, nan, clear, cloud = hls_data._set_train_columns_name()\n",
    "\n",
    "print('clear')\n",
    "display(clear)\n",
    "\n",
    "# train_data, test_data = hls_data._set_train_test_data(doy=211.0, x1=60.0, y1=40.0, x2=75.0, y2=55.0)\n",
    "\n",
    "train_data, test_data = hls_data._set_train_test_data(doy=211.0, x1=45.0, y1=45.0, x2=50.0, y2=50.0)\n",
    "# train_data, test_data = hls_data._set_timeseries_train_test_data(doy=211.0, x1=50.0, y1=50.0, x2=100.0, y2=100.0)\n",
    "\n",
    "print('train_data')\n",
    "display(train_data)\n",
    "print('test_data')\n",
    "display(test_data)\n",
    "# display(data)\n",
    "# display(nan)\n",
    "# display(clear)\n",
    "# display(cloud)\n",
    "\n",
    "hls_data._to_impute()\n",
    "hls_data._inference_train_test_data()\n",
    "hls_data._inference_imshow()\n",
    "\n",
    "# fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AHzEWLYbB-IY"
   },
   "outputs": [],
   "source": [
    "# data = train_data.copy() #[0:1000]\n",
    "data = train_data.copy()\n",
    "\n",
    "# table_dtype = 'int16'\n",
    "\n",
    "# data = data.astype(table_dtype)\n",
    "print(data.dtypes.tolist())\n",
    "# Reset the index to remove it\n",
    "data = data.reset_index(drop=True)\n",
    "# train_data.columns = final_columns_list\n",
    "display(data)\n",
    "\n",
    "\n",
    "# Split the DataFrame into training and test DataFrames\n",
    "# train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "# train_data = train_data.reset_index(drop=True)\n",
    "# display(train_data)\n",
    "# test_data = test_data.reset_index(drop=True)\n",
    "# display(test_data)\n",
    "\n",
    "EXP_NAME = 'exp-A100'\n",
    "EXP_NAME = 'exp-V100'\n",
    "EXP_NAME = 'exp-T4'\n",
    "\n",
    "######### SET OPTIMIZER FOR FIT ##########################\n",
    "optimizer_fit = 'sophia'\n",
    "##########################################################\n",
    "\n",
    "EXP_NAME = f'A0[great_{optimizer_fit}]'\n",
    "\n",
    "#### 1 ###########\n",
    "# learning_rate = 5e-5\n",
    "# lr_scheduler_type = 'constant' # constant_with_warmup\n",
    "\n",
    "\n",
    "######### SET TRAINER_RUN ARGUMENTS ##########################\n",
    "# learning_rate = 0.000025\n",
    "# lr_scheduler_type = 'cosine_with_restarts'\n",
    "# lr_scheduler_type = 'constant_with_warmup'\n",
    "# lr_scheduler_type = 'constant'\n",
    "# lr_scheduler_type = 'cosine'\n",
    "# lr_scheduler_type = 'linear'\n",
    "\n",
    "#init the optimizer\n",
    "# optimizer = SophiaG(model.parameters(), lr=2e-4, betas=(0.965, 0.99), rho = 0.01, weight_decay=1e-1)\n",
    "\n",
    "#### FOR V-100 GPU USABILITY################################\n",
    "epochs_steps = 1000   # 250 #50\n",
    "save_steps = 1000   #250 #50 #9380  #2000   # FOR FIT THIS VALUE START TRAIN DATASET, EQUALS FOR STEPS IN FIRST START\n",
    "logging_steps = 200 # 250 #25 #9380  #1000\n",
    "#### batch_size = 250 for T4\n",
    "batch_size = 200 #250 #312  # 224 # 250 #300 #164 for float64 # 112 # for float64 # 136 # 96   # 140# 400\n",
    "#######################################\n",
    "step_checkpoint_list = [7000, 105000, 82000, 32000, 22000]\n",
    "\n",
    "lr_fit=1e-5\n",
    "learning_rate = lr_fit\n",
    "\n",
    "experiment_dir = \"\"\n",
    "TRAINER_RUN = 5\n",
    "RESUME_FROM_CHECKPOINT = False\n",
    "\n",
    "\n",
    "if TRAINER_RUN == 0:\n",
    "    load_model_path = 'distilgpt2'\n",
    "    experiment_dir = f\"{EXP_NAME}/run[{TRAINER_RUN}]\"\n",
    "elif TRAINER_RUN > 0:\n",
    "    # before = pd.read_csv(f'{EXP_NAME}/run[{TRAINER_RUN-1}].csv')\n",
    "    # load_model_path = before['experiment_dir'][0] + f'/checkpoint-{step_checkpoint}'\n",
    "    if RESUME_FROM_CHECKPOINT == True:\n",
    "        step_checkpoint = step_checkpoint_list[TRAINER_RUN]\n",
    "        load_model_path = f'{EXP_NAME}/run[{TRAINER_RUN}]' + f'/checkpoint-{step_checkpoint}'\n",
    "        print(f'Resume from checkpoint:{load_model_path}')\n",
    "    else:\n",
    "        step_checkpoint = step_checkpoint_list[TRAINER_RUN-1]\n",
    "        load_model_path = f'{EXP_NAME}/run[{TRAINER_RUN-1}]' + f'/checkpoint-{step_checkpoint}'\n",
    "    experiment_dir  = f\"{EXP_NAME}/run[{TRAINER_RUN}]\"\n",
    "\n",
    "\n",
    "TRAINER_DICT = pd.DataFrame({'EXP_NAME' : EXP_NAME}, index = [TRAINER_RUN])    # dict()\n",
    "\n",
    "# lr_scheduler_type = ''\n",
    "TRAINER_DICT['TRAINER_RUN'] = TRAINER_RUN\n",
    "TRAINER_DICT['table_dtype'] = table_dtype\n",
    "TRAINER_DICT['epochs_steps'] = epochs_steps\n",
    "TRAINER_DICT['save_steps'] = save_steps\n",
    "TRAINER_DICT['logging_steps'] = logging_steps\n",
    "TRAINER_DICT['batch_size'] = batch_size\n",
    "TRAINER_DICT['learning_rate'] = learning_rate\n",
    "# TRAINER_DICT['lr_scheduler_type'] = lr_scheduler_type\n",
    "TRAINER_DICT['experiment_dir'] = experiment_dir\n",
    "TRAINER_DICT['optimizer_fit'] = optimizer_fit\n",
    "\n",
    "display(pd.DataFrame.from_dict(TRAINER_DICT))\n",
    "\n",
    "\n",
    "\n",
    "##### SET experiment_dir & efficient_finetuning #######################\n",
    "efficient_finetuning = ''  #'lora'\n",
    "if efficient_finetuning == 'lora':\n",
    "    experiment_dir = f\"{experiment_dir}_{efficient_finetuning}\"\n",
    "##### SET experiment_dir & efficient_finetuning #######################\n",
    "optimizer = 'adamw_torch'\n",
    "# optimizer = 'adamw_torch_fused'  #'adamw_torch'  #'adamw_torch_fused'\n",
    "if optimizer == 'adamw_torch_fused':\n",
    "    experiment_dir = f\"{experiment_dir}_{optimizer}\"\n",
    "#### SET model_save_dir through save_model #########################\n",
    "# model_save_dir = f'{EXP_NAME}/ZeroModel_{efficient_finetuning}'\n",
    "\n",
    "\n",
    "print('experiment_dir :', experiment_dir)\n",
    "print('load_model_path:', load_model_path)\n",
    "\n",
    "# Get the current CPU time in seconds since the epoch\n",
    "current_time = int(time.time())\n",
    "# Use the current time as a seed for a random number generator\n",
    "random_seed_state = current_time  # You can use this random_state for various random processes\n",
    "\n",
    "\n",
    "# load_model_path = 'load_model/checkpoint-16000'\n",
    "model = GReaT(llm=load_model_path,\n",
    "              batch_size=batch_size, epochs=epochs_steps, logging_steps=logging_steps, save_steps=save_steps,\n",
    "              # evaluation_strategy='steps',\n",
    "              # dataloader_num_workers=2, #fp16=True,\n",
    "              logging_first_step=True,\n",
    "              save_total_limit=2,\n",
    "              prediction_loss_only=True,\n",
    "              experiment_dir=f'{experiment_dir}',\n",
    "              dataloader_num_workers=2,\n",
    "              efficient_finetuning = efficient_finetuning,\n",
    "              learning_rate=learning_rate,\n",
    "              # lr_scheduler_type=lr_scheduler_type,\n",
    "              warmup_steps = 5000,\n",
    "              # warmup_ratio=0.05,\n",
    "              seed=current_time,\n",
    "              data_seed=current_time+int(time.time()),\n",
    "              # optim=optimizer,\n",
    "              fp16 = True,\n",
    "              # torch_compile=True,   #### uncomment for Ampere\n",
    "              # bf16=True,            #### uncomment for Ampere\n",
    "              )\n",
    "\n",
    "print(f'----------- Model architecture, efficient_finetuning: {efficient_finetuning} -----------------------')\n",
    "print(model.model)\n",
    "print(f'----------------------------------------------------------------------------------------------------')\n",
    "# fn\n",
    "\n",
    "if RESUME_FROM_CHECKPOINT == True:\n",
    "    model.fit(data=data, optimizer=optimizer_fit, lr_fit=lr_fit, resume_from_checkpoint=True)\n",
    "else:\n",
    "    model.fit(data=data, optimizer=optimizer_fit, lr_fit=lr_fit)\n",
    "# model.fit(data=train_data, test_data=test_data)\n",
    "\n",
    "model.save(f'{experiment_dir}/model')\n",
    "\n",
    "TRAINER_DICT.to_csv(f'{experiment_dir}.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LV037qqBNdol"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "695cBCyoBG3q"
   },
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9v8lVF5Sayy"
   },
   "outputs": [],
   "source": [
    "fn"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
