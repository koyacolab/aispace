{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1698346597091,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "Qj506dTf0hD4"
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18320,
     "status": "ok",
     "timestamp": 1698346615405,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "tkYBYiue0lS7",
    "outputId": "fc36a567-f523-4e53-9362-e88287f10382"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')\n",
    "\n",
    "# import os\n",
    "\n",
    "# # !pip install fire\n",
    "# # !pip install tqdm\n",
    "\n",
    "# home_dir = '/content/gdrive/My Drive/A0/aispace'\n",
    "# os.chdir(home_dir)\n",
    "# !pwd\n",
    "\n",
    "# import os\n",
    "# # Get the current working directory\n",
    "# current_directory = os.getcwd()\n",
    "# print(current_directory)\n",
    "\n",
    "# import shutil\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46999,
     "status": "ok",
     "timestamp": 1698346662401,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "pEOnhw3U0hD6",
    "outputId": "729c0b36-a21c-4a7c-b2a7-780072db6e4a"
   },
   "outputs": [],
   "source": [
    "# !pip install rasterio\n",
    "# !pip install accelerate\n",
    "# !pip install peft\n",
    "# !pip install transformers\n",
    "# # !pip install transformers==4.33.0\n",
    "# !pip install datasets\n",
    "# # !python3 -m pip install --upgrade pip\n",
    "# # !pip install Sophia-Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1698346662402,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "3_zkf2dM0hD6"
   },
   "outputs": [],
   "source": [
    "KAGGLE = False\n",
    "if KAGGLE == True:\n",
    "    # Define the input and output directories\n",
    "    input_directory  = '/kaggle/input/aidataset'  # Replace with the path to your input directory\n",
    "    output_directory = '/kaggle/working'  # Replace with the path to your output directory\n",
    "\n",
    "    def input_copy(input_directory, output_directory):\n",
    "        # Get a list of files in the input directory\n",
    "        files_to_copy = os.listdir(input_directory)\n",
    "        # Iterate through the files and copy them to the output directory\n",
    "        for file_name in files_to_copy:\n",
    "            # Create the full paths for the source and destination\n",
    "            source_file = os.path.join(input_directory, file_name)\n",
    "            destination_file = os.path.join(output_directory, file_name)\n",
    "\n",
    "            # Copy the file from the source to the destination\n",
    "            shutil.copy(source_file, destination_file)\n",
    "\n",
    "        # Get a list of files in the input directory\n",
    "        files = os.listdir(output_directory)\n",
    "        print(files)\n",
    "\n",
    "    input_copy(input_directory, output_directory)\n",
    "\n",
    "    source_directory  = '/kaggle/input/'  # Replace with the path to your input directory\n",
    "    destination_directory = '/kaggle/working/input'  # Replace with the path to your output directory\n",
    "\n",
    "    # Copy the source directory to the destination directory\n",
    "    shutil.copytree(source_directory, destination_directory)\n",
    "\n",
    "    input_copy(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1698346662402,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "5fzTgKJLQtKK"
   },
   "outputs": [],
   "source": [
    "# import sophia\n",
    "\n",
    "# !pip install wandb\n",
    "\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17558,
     "status": "ok",
     "timestamp": 1698346679956,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "REzgouGF0hD7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import great\n",
    "from great import GReaT\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "################################\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# all imports should go here\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import skimage.exposure\n",
    "\n",
    "# access package for AWS access\n",
    "# import boto3\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import datetime\n",
    "import platform\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import ee\n",
    "import h5py\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta  # Import timedelta here\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import rasterio as rio\n",
    "################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from hlsdataset import HLSDataSet\n",
    "\n",
    "import shutil\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Execute only once!\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6052,
     "status": "ok",
     "timestamp": 1698346685999,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "XtgFFiGF0hD7",
    "outputId": "42406310-0b1a-450f-e32d-77dae7933fd1"
   },
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "\n",
    "print(f'NumPy version:{np.__version__}')\n",
    "np.float = float # 'float32' # float\n",
    "table_dtype = int  # np.float  # 'int64'   # np.float  #'float32'\n",
    "print(table_dtype)\n",
    "\n",
    "hls_data = HLSDataSet(table_dtype = table_dtype)\n",
    "\n",
    "# hls_data._REFLECTANCE(round=3)\n",
    "hls_data._QUANTIZATE(round=3)\n",
    "\n",
    "#### clip 50x50 subdataset\n",
    "# hls_data.clip_dataset(x1=50.0, y1=50.0, x2=100.0, y2=100.0)\n",
    "\n",
    "# doys = [171, 179, 187, 195, 203, 211, 219]\n",
    "\n",
    "# doys = [171, 179, 187, 203, 211, 219]\n",
    "\n",
    "doys = [171, 187, 203, 211, 219]\n",
    "# doys = [203, 211, 219]\n",
    "# doys = [203, 211, 219]\n",
    "\n",
    "df = hls_data._get_data_doys(doys = doys, SHOW=True)\n",
    "display(df)\n",
    "\n",
    "# df = hls_data._set_columns_name()\n",
    "# display(df)\n",
    "\n",
    "df1, df2 = hls_data._nan_9999()\n",
    "# display(df1)\n",
    "# display(df2)\n",
    "\n",
    "df1, df2 = hls_data._set_clear_cloud()\n",
    "# display(df1)\n",
    "# display(df2)\n",
    "\n",
    "data, nan, clear, cloud = hls_data._set_train_columns_name()\n",
    "\n",
    "print('clear')\n",
    "display(clear)\n",
    "\n",
    "# train_data, test_data = hls_data._set_train_test_data(doy=211.0, x1=60.0, y1=40.0, x2=75.0, y2=55.0)\n",
    "\n",
    "# train_data, test_data = hls_data._set_train_test_data(doy=211.0, x1=45.0, y1=45.0, x2=50.0, y2=50.0)\n",
    "\n",
    "train_data, test_data = hls_data._set_train_test_data(doy=211.0, x1=95.0, y1=0.0, x2=100.0, y2=5.0, for_show_nan=False)\n",
    "\n",
    "# train_data, test_data = hls_data._set_train_test_data(doy=187.0, x1=2.0, y1=0.0, x2=21.0, y2=50.0, for_show_nan=False)\n",
    "# train_data, test_data = hls_data._set_timeseries_train_test_data(doy=211.0, x1=50.0, y1=50.0, x2=100.0, y2=100.0)\n",
    "\n",
    "print('train_data:')\n",
    "display(train_data)\n",
    "print('test_data:')\n",
    "display(test_data)\n",
    "# display(data)\n",
    "# display(nan)\n",
    "# display(clear)\n",
    "# display(cloud)\n",
    "\n",
    "hls_data._to_impute()\n",
    "hls_data._inference_train_test_data()\n",
    "hls_data._inference_imshow()\n",
    "\n",
    "# fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_llm(train_data, train_columns_list = ['DOY', 'PID', 'BAND']):\n",
    "    ####### COMBINE ALL CHANNELS IN ONE BAND ################################################################\n",
    "    # Create a new column 'BAND' by concatenating 'B02', 'B03', and 'B04' with ';'\n",
    "    # train_data['BAND'] = train_data.apply(lambda row: ';'.join([str(row['B02']), str(row['B03']), str(row['B04'])]), axis=1)\n",
    "    \n",
    "    # Combine 'B02', 'B03', and 'B04' to the 'BAND' column as a string with ';' and ';'\n",
    "    train_data['BAND'] = train_data[['B02', 'B03', 'B04']].astype(str).agg(lambda x: ';'.join(x) + ';', axis=1)\n",
    "    \n",
    "    # # Inverse operation: Split 'BAND' column back into 'B02', 'B03', and 'B04'\n",
    "    # df[['B02', 'B03', 'B04']] = df['BAND'].str.rstrip(';').str.split(';', expand=True)\n",
    "    \n",
    "    #### CODE DATA 4 TRAIN #####################################\n",
    "    train_columns_list = ['DOY', 'PID', 'BAND']\n",
    "    # train_columns_list = ['DOY', 'PID', 'B02', 'B03', 'B04']\n",
    "    # train_columns_list = ['B02', 'B03', 'B04', 'X', 'Y', 'DOY']\n",
    "    # train_columns_list = ['B03', 'PID', 'DOY']\n",
    "    data = train_data[train_columns_list].copy()\n",
    "    \n",
    "    # data['B02'] = data['B02'] * 1000\n",
    "    # data['B03'] = data['B03'] * 1000\n",
    "    # data['B04'] = data['B04'] * 1000\n",
    "    # data = data.astype(int)\n",
    "    \n",
    "    display(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "train_columns_list = ['DOY', 'PID', 'BAND'] \n",
    "\n",
    "print('data:')\n",
    "data     = to_llm(train_data, \n",
    "                  train_columns_list = train_columns_list)\n",
    "\n",
    "print('img_data:')\n",
    "img_data = to_llm(test_data,  \n",
    "                  train_columns_list = train_columns_list)\n",
    "\n",
    "# # Create a new DataFrame with four columns\n",
    "# data = data.melt(id_vars=['PID', 'DOY'], value_vars=['B02', 'B03', 'B04'],\n",
    "#                     var_name='BNAME', value_name='BAND')\n",
    "\n",
    "# data['BNAME'] = data['BNAME'].astype(str)\n",
    "# #############################################################\n",
    "\n",
    "# # Get only trainable columns ###########################\n",
    "# train_columns_list = ['DOY', 'PID', 'BNAME', 'BAND']\n",
    "\n",
    "# data = data[train_columns_list].copy()\n",
    "\n",
    "# # data = data.astype(table_dtype)\n",
    "# print(data.dtypes.tolist())\n",
    "# # Reset the index to remove it\n",
    "# data = data.reset_index(drop=True)\n",
    "# # train_data.columns = final_columns_list\n",
    "# print(f'Code data 4 trainable is for {train_columns_list}')\n",
    "# display(data)\n",
    "\n",
    "# print('Ready for train...')\n",
    "# fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### TRAIN TOKENIZER FOR HLS DATA ################################################\n",
    "# #### TRAIN ONCE AND COMMENT FOR AVOID FORKING TOKENIZER ##########################\n",
    "\n",
    "EXP_NAME = f'A0[RGB_TOK[ai]_Lora]'\n",
    "EXP_NAME = f'A0[RGB_REFL[int]BAND]'\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()\n",
    "\n",
    "ReTRAIN_TOKENIZER = False\n",
    "if ReTRAIN_TOKENIZER == True:\n",
    "    def _get_synth_data():\n",
    "        BAND = [b for b in range(0, 10)]\n",
    "        PID  = [p for p in range(0, 10)]\n",
    "        DOY  = [d for d in range(0, 10)]   \n",
    "        # BNAME = ['B02', 'B03', 'B04']\n",
    "        # Create a DataFrame with specified column names\n",
    "        # synth_data = pd.DataFrame(list(zip(BAND, PID, DOY)), columns=['BAND', 'PID', 'DOY'])\n",
    "        synth_data = pd.DataFrame(list(zip(BAND, BAND, BAND, PID, DOY)), columns=['B02', 'B03', 'B04', 'PID', 'DOY'])\n",
    "        # synth_data = pd.DataFrame(list(zip(BAND, BAND, BAND, PID, PID, DOY)), columns=['B02', 'B03', 'B04', 'X', 'Y', 'DOY'])  \n",
    "\n",
    "        # # Create a list of tuples with all possible combinations of DOY and PID\n",
    "        # synth_data = [(d, p, b) for d in DOY for p in PID for b in BAND]\n",
    "        # # Create a DataFrame from the data\n",
    "        # synth_data = pd.DataFrame(synth_data, columns=[\"DOY\", \"PID\", \"B03\"])\n",
    "        return synth_data.astype(table_dtype)\n",
    "    \n",
    "    synth_data = _get_synth_data()\n",
    "\n",
    "    synth_data = data\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    display(synth_data)\n",
    "    \n",
    "    from transformers import AutoTokenizer\n",
    "    from aitokenizer import TrainTokenizer, TrainCorpus, PreTrainTokenizer\n",
    "    \n",
    "    old_tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "    old_tokenizer.pad_token = old_tokenizer.eos_token\n",
    "    \n",
    "    data_corpus = TrainCorpus(synth_data)._get_training_corpus()\n",
    "\n",
    "    sample = next(data_corpus)[0]\n",
    "    print('L8 data:', sample)\n",
    "    \n",
    "    tokens = old_tokenizer.tokenize(sample)\n",
    "    print('old_tokenizer:', len(tokens), tokens)\n",
    "    # print(old_tokenizer(sample).tokens())\n",
    "\n",
    "    digits_tok = [str(x) for x in range(0,10)]\n",
    "    \n",
    "    new_tokenizer = PreTrainTokenizer(data, old_tokenizer)._train(special_tokens=digits_tok+train_columns_list+[';',])\n",
    "    \n",
    "    # print(new_tokenizer(sample).tokens())\n",
    "    tokens = new_tokenizer.tokenize(sample)\n",
    "    print('aispace-tokenizer:', len(tokens), tokens)\n",
    "    \n",
    "    new_tokenizer.save_pretrained(f\"{EXP_NAME}/aispace-tokenizer\")\n",
    "\n",
    "    print('Tokenizer trained, COMMENT & RELOAD Tokenizer Trainer...')\n",
    "\n",
    "# else:\n",
    "#     new_tokenizer = AutoTokenizer.from_pretrained(f\"{EXP_NAME}/aispace-tokenizer\")\n",
    "#     new_tokenizer.pad_token = new_tokenizer.eos_token\n",
    "\n",
    "#     data_corpus = TrainCorpus(data)._get_training_corpus()\n",
    "\n",
    "#     sample = next(data_corpus)[0]\n",
    "#     print('L8 data:', sample)\n",
    "\n",
    "#     tokens = old_tokenizer.tokenize(sample)\n",
    "#     print('old_tokenizer:', tokens)\n",
    "\n",
    "    fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AHzEWLYbB-IY"
   },
   "outputs": [],
   "source": [
    "#### LISTS for TRAINER_RUN cycles COSINE WITH WARM UP ###################################\n",
    "epochs_list = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "max_steps_list =    [200000, 200000, 100000]\n",
    "warmup_steps_list = [100000,  10000,  0]\n",
    "\n",
    "lr_scheduler_type_list = ['polynomial', 'polynomial', 'polynomial']     # 'polynomial'      \n",
    "num_cycles = 5\n",
    "\n",
    "learning_rate_list = [2e-8, 5e-7, 8e-7]\n",
    "########################################################################\n",
    "step_checkpoint_list = [200000, 100000, 100000]   # [16000, 175000, 175000, 175000, 175000]\n",
    "\n",
    "# #### LISTS for TRAINER_RUN cycles POLYNOMIAL WITH WARMUP ###################################\n",
    "# epochs_list = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# max_steps_list =    [20000, 10000, 10000, 20000, 20000, 20000, 20000, 20000]\n",
    "# warmup_steps_list = [15000,  5000,  5000,      0,     0,     0,     0,     0]\n",
    "\n",
    "# lr_scheduler_type_list = ['constant', 'constant', 'constant', 'polynomial', 'polynomial', 'polynomial', 'polynomial', 'polynomial' ]     # 'polynomial'      \n",
    "# num_cycles = 3\n",
    "\n",
    "# learning_rate_list = [1e-7, 1e-6, 1e-4, 1e-4, 7e-5, 2e-5, 7e-6, 2e-6]\n",
    "# ########################################################################\n",
    "# step_checkpoint_list = max_steps_list   # [16000, 175000, 175000, 175000, 175000]\n",
    "\n",
    "# os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ['HF_EVALUATE_OFFLINE'] = '1'\n",
    "\n",
    "TRAINER_RUN = 1\n",
    "# for TRAINER_RUN in range( len(lr_scheduler_type_list) ):\n",
    "RESUME_FROM_CHECKPOINT = False\n",
    "\n",
    "# wandb.login(relogin=True, force=True, key='10a24a9f6330ab3c52f92ce91d927df68c6058ac')  #--relogin\n",
    "\n",
    "\n",
    "#### SET MODEL LOADING FROM: 0 - 'distilgpt2', else - checkpoint-'' \n",
    "if TRAINER_RUN == 0:\n",
    "    load_model_path = 'distilgpt2'\n",
    "    experiment_dir = f\"{EXP_NAME}/run[{TRAINER_RUN}]\"\n",
    "elif TRAINER_RUN > 0:\n",
    "    # before = pd.read_csv(f'{EXP_NAME}/run[{TRAINER_RUN-1}].csv')\n",
    "    # load_model_path = before['experiment_dir'][0] + f'/checkpoint-{step_checkpoint}'\n",
    "    if RESUME_FROM_CHECKPOINT == True:\n",
    "        step_checkpoint = step_checkpoint_list[TRAINER_RUN]\n",
    "        load_model_path = f'{EXP_NAME}/run[{TRAINER_RUN}]' + f'/checkpoint-{step_checkpoint}'\n",
    "        print(f'Resume from checkpoint:{load_model_path}')\n",
    "    else:\n",
    "        step_checkpoint = step_checkpoint_list[TRAINER_RUN-1]\n",
    "        load_model_path = f'{EXP_NAME}/run[{TRAINER_RUN-1}]' + f'/checkpoint-{step_checkpoint}'\n",
    "    experiment_dir  = f\"{EXP_NAME}/run[{TRAINER_RUN}]\"\n",
    "\n",
    "###########################################################\n",
    "#### TRAINER HYPERPARAMETERS #############################\n",
    "save_steps = 1000\n",
    "logging_steps = 1000    # 200\n",
    "\n",
    "batch_size = 280  \n",
    "batch_size = 3600  # 180 #for 60 tokens\n",
    "\n",
    "epochs = epochs_list[TRAINER_RUN]    # ignored, use max_steps     \n",
    "max_steps = max_steps_list[TRAINER_RUN] # if -1 max_steps ignored \n",
    "\n",
    "learning_rate = learning_rate_list[TRAINER_RUN]\n",
    "\n",
    "lr_scheduler_type = lr_scheduler_type_list[TRAINER_RUN]\n",
    "\n",
    "warmup_steps = warmup_steps_list[TRAINER_RUN]\n",
    "\n",
    "optimizer = 'sophia'\n",
    "###########################################################\n",
    "##### SET experiment_dir & efficient_finetuning #######################\n",
    "efficient_finetuning = ''  #'lora'\n",
    "##########################################################\n",
    "###### WANDB CONFIG ############################\n",
    "# 🐝 1️⃣ Start a new run to track this script\n",
    "wandb.init(\n",
    "  # Set the project where this run will be logged\n",
    "  project=\"aispace\", \n",
    "  # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "  name=f'{EXP_NAME}/run[{TRAINER_RUN}]', \n",
    "  reinit=True,\n",
    "  force=True,\n",
    "  # magic=True,\n",
    "  # Track hyperparameters and run metadata\n",
    "  # config={\n",
    "  # # \"learning_rate\": learning_rate,\n",
    "  # \"architecture\": \"GPT2\",\n",
    "  # \"dataset\": \"HLS/L-8\",\n",
    "  # # \"max_steps\": max_steps,\n",
    "  # })   \n",
    "  )\n",
    "###########################################################\n",
    "\n",
    "TRAINER_DICT = pd.DataFrame({'EXP_NAME' : EXP_NAME}, index = [TRAINER_RUN])    # dict()\n",
    "TRAINER_DICT['TRAINER_RUN'] = TRAINER_RUN\n",
    "TRAINER_DICT['table_dtype'] = table_dtype\n",
    "TRAINER_DICT['experiment_dir'] = experiment_dir\n",
    "TRAINER_DICT['save_steps'] = save_steps\n",
    "TRAINER_DICT['logging_steps'] = logging_steps\n",
    "TRAINER_DICT['epochs'] = epochs\n",
    "TRAINER_DICT['max_steps'] = max_steps\n",
    "TRAINER_DICT['batch_size'] = batch_size\n",
    "TRAINER_DICT['learning_rate'] = learning_rate\n",
    "TRAINER_DICT['lr_scheduler_type'] = lr_scheduler_type\n",
    "TRAINER_DICT['warmup_steps'] = warmup_steps\n",
    "TRAINER_DICT['num_cycles'] = num_cycles\n",
    "TRAINER_DICT['optimizer'] = optimizer\n",
    "TRAINER_DICT['fp16'] = True\n",
    "\n",
    "display(pd.DataFrame.from_dict(TRAINER_DICT))\n",
    "\n",
    "print('experiment_dir :', experiment_dir)\n",
    "print('load_model_path:', load_model_path)\n",
    "\n",
    "# Get the current CPU time in seconds since the epoch\n",
    "current_time = int(time.time())\n",
    "# Use the current time as a seed for a random number generator\n",
    "random_seed_state = current_time  # You can use this random_state for various random processes\n",
    "\n",
    "model = GReaT(llm=load_model_path,\n",
    "              # tokenizer=load_model_path,\n",
    "              tokenizer=f\"{EXP_NAME}/aispace-tokenizer\",\n",
    "              # tokenizer=load_model_path,\n",
    "              auto_find_batch_size=True,\n",
    "              batch_size=batch_size, epochs=epochs, max_steps = max_steps,\n",
    "              logging_steps=logging_steps, save_steps=save_steps,\n",
    "              # evaluation_strategy='steps',\n",
    "              logging_first_step=True,\n",
    "              save_total_limit=2,\n",
    "              # prediction_loss_only=True,\n",
    "              experiment_dir=experiment_dir,\n",
    "              dataloader_num_workers=2,\n",
    "              efficient_finetuning = efficient_finetuning,\n",
    "              learning_rate=learning_rate,\n",
    "              lr_scheduler_type=lr_scheduler_type,\n",
    "              warmup_steps = warmup_steps,\n",
    "              num_cycles = num_cycles,\n",
    "              # warmup_ratio=0.05,\n",
    "              seed=current_time,\n",
    "              data_seed=current_time+int(time.time()),\n",
    "              # optim=TRAINER_DICT['optimizer'],\n",
    "              fp16 = True,            #### comment for Ampere, for Volta\n",
    "              # torch_compile=True,   #### uncomment for Ampere\n",
    "              # bf16=True,            #### uncomment for Ampere\n",
    "              report_to='wandb',\n",
    "              run_name=f'{EXP_NAME}/run[{TRAINER_RUN}]',\n",
    "              evaluation_strategy=\"steps\",\n",
    "              eval_steps=logging_steps,\n",
    "              label_names=[\"DOY\", \"PID\", \"BAND\"] # \"B02\", \"B03\", \"B04\"],    \n",
    "              )\n",
    "\n",
    "# print(f'----------- Model architecture, efficient_finetuning: {efficient_finetuning} -----------------------')\n",
    "# print(model.model)\n",
    "# print(f'----------------------------------------------------------------------------------------------------')\n",
    "# fn\n",
    "\n",
    "# Split the DataFrame into training and testing datasets (80% train, 20% test)\n",
    "data_train, data_test = train_test_split( data, test_size=0.1, random_state=42 )  #, shuffle=False)\n",
    "\n",
    "# data_train = data.reset_index(drop=True).copy()\n",
    "\n",
    "data_train = data_train[train_columns_list].reset_index(drop=True).copy()\n",
    "data_test = data_test[train_columns_list].reset_index(drop=True).copy()\n",
    "\n",
    "if RESUME_FROM_CHECKPOINT == True:\n",
    "    model.fit(data=data, test_data=data_test, resume_from_checkpoint=True)\n",
    "else:\n",
    "    model.fit(data=data_train, test_data=data_test, conditional_col='DOY')\n",
    "    # model.fit(data=data, test_data=test_data.copy())\n",
    "\n",
    "# model.save(f'{experiment_dir}/model')\n",
    "\n",
    "TRAINER_DICT.to_csv(f'{experiment_dir}.csv')\n",
    "\n",
    "# Mark the run as finished\n",
    "wandb.finish(exit_code=99, quiet=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LV037qqBNdol"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "695cBCyoBG3q"
   },
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "f9v8lVF5Sayy"
   },
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
