{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1698373996251,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "Qj506dTf0hD4"
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22764,
     "status": "ok",
     "timestamp": 1698374019009,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "tkYBYiue0lS7",
    "outputId": "6afc71ac-0294-4947-8757-067dff387555"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')\n",
    "\n",
    "# import os\n",
    "\n",
    "# # !pip install fire\n",
    "# # !pip install tqdm\n",
    "\n",
    "# home_dir = '/content/gdrive/My Drive/A0/aispace'\n",
    "# os.chdir(home_dir)\n",
    "# !pwd\n",
    "\n",
    "# import os\n",
    "# # Get the current working directory\n",
    "# current_directory = os.getcwd()\n",
    "# print(current_directory)\n",
    "\n",
    "# import shutil\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42824,
     "status": "ok",
     "timestamp": 1698374061828,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "pEOnhw3U0hD6",
    "outputId": "33c8cc60-24a7-4ff8-9fcb-fc884c7a24ba"
   },
   "outputs": [],
   "source": [
    "# !pip install rasterio\n",
    "# !pip install accelerate\n",
    "# !pip install peft\n",
    "# !pip install transformers\n",
    "# # !pip install transformers==4.34.0\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(evaluate\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mfn\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fn' is not defined"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "print(evaluate.__version__)\n",
    "# fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1698374061829,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "3_zkf2dM0hD6"
   },
   "outputs": [],
   "source": [
    "KAGGLE = False\n",
    "if KAGGLE == True:\n",
    "    # Define the input and output directories\n",
    "    input_directory  = '/kaggle/input/begreat'  # Replace with the path to your input directory\n",
    "    output_directory = '/kaggle/working'  # Replace with the path to your output directory\n",
    "\n",
    "    def input_copy(input_directory, output_directory):\n",
    "        # Get a list of files in the input directory\n",
    "        files_to_copy = os.listdir(input_directory)\n",
    "        # Iterate through the files and copy them to the output directory\n",
    "        for file_name in files_to_copy:\n",
    "            # Create the full paths for the source and destination\n",
    "            source_file = os.path.join(input_directory, file_name)\n",
    "            destination_file = os.path.join(output_directory, file_name)\n",
    "\n",
    "            # Copy the file from the source to the destination\n",
    "            shutil.copy(source_file, destination_file)\n",
    "\n",
    "        # Get a list of files in the input directory\n",
    "        files = os.listdir(output_directory)\n",
    "        print(files)\n",
    "\n",
    "    input_copy(input_directory, output_directory)\n",
    "\n",
    "    source_directory  = '/kaggle/input/'  # Replace with the path to your input directory\n",
    "    destination_directory = '/kaggle/working/input'  # Replace with the path to your output directory\n",
    "\n",
    "    # Copy the source directory to the destination directory\n",
    "    shutil.copytree(source_directory, destination_directory)\n",
    "\n",
    "    input_copy(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22684,
     "status": "ok",
     "timestamp": 1698374084509,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "REzgouGF0hD7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import great\n",
    "from great import GReaT\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "################################\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# all imports should go here\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import skimage.exposure\n",
    "\n",
    "# access package for AWS access\n",
    "# import boto3\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import datetime\n",
    "import platform\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import ee\n",
    "import h5py\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta  # Import timedelta here\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import rasterio as rio\n",
    "################################\n",
    "\n",
    "from hlsdataset import HLSDataSet\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Execute only once!\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1698374084510,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "1ymo4JOO2iK9"
   },
   "outputs": [],
   "source": [
    "class HLSInference(HLSDataSet):\n",
    "    def __init__(self, doy=211, table_dtype = 'float16', path='./aispace/data/L8-100x100'):\n",
    "        super().__init__(table_dtype = table_dtype, path=path)\n",
    "        # self.model = model\n",
    "        self.doy_to_impute = doy\n",
    "\n",
    "        self.imputed_data = None\n",
    "        self.recovered_data = None\n",
    "\n",
    "        self.imputed_file = ''\n",
    "\n",
    "        # self.nan_data = self.to_impute #.copy()    # to_impute.copy()\n",
    "        # display(self.nan_data)\n",
    "        #### for only one day processing ###########################\n",
    "        self.nan_data_doy = None # self.nan_data.loc[(self.nan_data['DOY'] == doy)]\n",
    "        self.nan_data_doy = None #  self.nan_data_doy.reset_index(drop=True)\n",
    "\n",
    "        self.data_doy = None #  self.clear_data.loc[(self.clear_data['DOY'] == doy)]\n",
    "        self.data_doy = None #  self.data_doy.reset_index(drop=True)\n",
    "\n",
    "        # self.nan_data_resid  = self.nan_data.loc[(nan_data['DOY'] != doy)]\n",
    "        # self.nan_data_resid  = self.nan_data_resid.reset_index(drop=True)\n",
    "\n",
    "    def _impute(self, model, columns_impute=['B02', 'B03', 'B04', 'PID', 'DOY'], device='cuda', k=400, max_length=1000, temperature=0.01):\n",
    "    # def _impute(self, model, columns_impute=['B02', 'B03', 'B04', 'X', 'Y', 'DOY'], device='cuda', k=400, max_length=1000, temperature=0.01):\n",
    "\n",
    "        self.nan_data_doy = self.to_impute.loc[(self.to_impute['DOY'] == self.doy_to_impute)]\n",
    "        self.nan_data_doy = self.nan_data_doy.reset_index(drop=True)\n",
    "\n",
    "        self.data_doy = self.clear_data.loc[(self.clear_data['DOY'] == self.doy_to_impute)]\n",
    "        self.data_doy = self.data_doy.reset_index(drop=True)\n",
    "\n",
    "        # print(f'to impute data:')\n",
    "        # display(self.nan_data_doy)\n",
    "        print(f'to impute data:')\n",
    "        display(self.nan_data_doy)\n",
    "        display(self.data_doy)\n",
    "\n",
    "        # fn\n",
    "\n",
    "        print(f'NumPy version:{np.__version__}')\n",
    "        np.float = float\n",
    "\n",
    "        # Get the DataFrame with columns in reverse order\n",
    "        # self.nan_data_doy = self.nan_data_doy[self.nan_data_doy.columns[::-1]].copy()\n",
    "\n",
    "        ######################### ORIGINAL ##############################################################\n",
    "        _impute = self.nan_data_doy[columns_impute].copy()\n",
    "        # _impute[['DOY', 'PID']] = _impute[['DOY', 'PID']].astype(int)\n",
    "        print('IMPUTE:')\n",
    "        display(_impute)\n",
    "        # self.imputed_data = pd.read_csv('imputed.csv')\n",
    "        self.imputed_data = model.impute(_impute, k=k, max_length=max_length, temperature=temperature, device=device)\n",
    "        self.imputed_data.to_csv('imputed2.csv')\n",
    "\n",
    "        print('IMPUTED:')\n",
    "        display(self.imputed_data)\n",
    "        #########################################################################################\n",
    "        # ################################## FOR BANDS in ONE BAND#####################################################\n",
    "        # # _impute = self.nan_data_doy[columns_impute].copy()\n",
    "        # _impute = self.nan_data_doy[columns_impute].copy()\n",
    "        # _impute[['PID', 'DOY', 'BAND']] = self.nan_data_doy[['PID', 'DOY', 'B03']].copy()\n",
    "        # _impute = _impute[['PID', 'DOY', 'BAND']].copy()\n",
    "        # print('IMPUTE:')\n",
    "        # display(_impute)\n",
    "        # # self.imputed_data = pd.read_csv('imputed.csv')\n",
    "        # self.imputed_data = model.impute(_impute, k=k, max_length=max_length, temperature=temperature, device=device)\n",
    "        # self.imputed_data.to_csv('imputed2.csv')\n",
    "\n",
    "        # print('IMPUTED:')\n",
    "        # display(self.imputed_data)\n",
    "\n",
    "        # # # Inverse operation: Split 'BAND' column back into individual columns\n",
    "        # aa = self.nan_data_doy[columns_impute].copy()\n",
    "        # display(aa)\n",
    "        \n",
    "        # # aa[['B02', 'B03', 'B04', 'NN']]  \n",
    "        # aa = self.imputed_data['BAND'].str.split(';', expand=True)\n",
    "        # display(aa)\n",
    "        \n",
    "        # self.imputed_data[['B02', 'B03', 'B04', 'NN']] = self.imputed_data['BAND'].str.split(';', expand=True)\n",
    "\n",
    "        # # # Convert columns back to their original data types\n",
    "        # self.imputed_data[['B02', 'B03', 'B04']] = self.imputed_data[['B02', 'B03', 'B04']].astype(table_dtype)\n",
    "\n",
    "        # print('IMPUTED:')\n",
    "        # display(self.imputed_data)\n",
    "        # #########################################################################################\n",
    "        \n",
    "        # imputed_file = f'A0[optim_sophia]/imputed_output_run[3].csv'\n",
    "        # self.imputed_data = pd.read_csv(imputed_file)\n",
    "        # self.imputed_data = self.nan_data_doy.copy()\n",
    "\n",
    "        print(self.nan_data_doy.columns, self.imputed_data.columns)\n",
    "        print(self.nan_data_doy.shape, self.imputed_data.shape)\n",
    "\n",
    "        # Merge the dataframes by X and Y columns and replace B3 in df1 with B3 from df2\n",
    "        merged_df = self.nan_data_doy.merge(self.imputed_data, on=['PID', 'DOY'], suffixes=('', '_df2'), how='left')  \n",
    "        # Replace the original B3 column with B3 from df2\n",
    "        merged_df['B02'] = merged_df['B02_df2']\n",
    "        merged_df['B03'] = merged_df['B03_df2']\n",
    "        merged_df['B04'] = merged_df['B04_df2']\n",
    "        \n",
    "        # Drop the additional B3_df2 column\n",
    "        merged_df = merged_df.drop('B02_df2', axis=1)\n",
    "        merged_df = merged_df.drop('B03_df2', axis=1)\n",
    "        merged_df = merged_df.drop('B04_df2', axis=1)\n",
    "\n",
    "        self.imputed_data = merged_df.copy()\n",
    "        \n",
    "        print(self.nan_data_doy.columns, self.imputed_data.columns)\n",
    "        print(self.nan_data_doy.shape, self.imputed_data.shape)\n",
    "        # fn\n",
    "        # cols = self.nan_data_doy.columns \n",
    "        # self.imputed_data\n",
    "\n",
    "        # Get the DataFrame with columns in reverse order\n",
    "        self.imputed_data = self.imputed_data[self.imputed_data.columns[::-1]].copy()\n",
    "\n",
    "        if len(self.imputed_data) != len(self.nan_data_doy):\n",
    "            print('len(self.imputed_data) != len(self.nan_data_doy)')\n",
    "            # Use the merge function with indicator=True\n",
    "            original_df = self.nan_data_doy\n",
    "            subset_df = self.imputed_data\n",
    "\n",
    "            merged_df = pd.merge(original_df, subset_df, on=['PID', 'DOY'], how='left', indicator=True)\n",
    "\n",
    "            # Find the rows in original_df that are not in subset_df\n",
    "            missing_rows = original_df[merged_df['_merge'] == 'left_only']\n",
    "\n",
    "            # Display the missing rows\n",
    "            # display(missing_rows)\n",
    "\n",
    "            self.imputed_data = pd.concat([self.imputed_data, missing_rows], axis=0)\n",
    "\n",
    "        self.recovered_data = pd.concat([self.imputed_data, self.data_doy], axis=0)\n",
    "        self.recovered_data = self.recovered_data.reset_index(drop=True)\n",
    "\n",
    "        print('imputed_data')\n",
    "        display(self.imputed_data)\n",
    "\n",
    "        print('recovered_data')\n",
    "        display(self.recovered_data)\n",
    "\n",
    "        return self.recovered_data\n",
    "\n",
    "    def _set_inference_recovered(self,):\n",
    "       self.inference_data = self.recovered_data\n",
    "       self.inference_data = self.inference_data.sort_values(by=['Y', 'X', 'DOY', ])\n",
    "\n",
    "       return self.inference_data\n",
    "\n",
    "    def _save_recovered(self, imputed_file=f'recovered_output.csv'):\n",
    "        self.imputed_file = imputed_file\n",
    "        print(imputed_file)\n",
    "        self.recovered_data.to_csv(self.imputed_file)\n",
    "\n",
    "    def _read_recovered(self, imputed_file=f'recovered_output.csv'):\n",
    "        self.imputed_file = imputed_file\n",
    "        print(imputed_file)\n",
    "        self.recovered_data = pd.read_csv(self.imputed_file)\n",
    "        # display(self.imputed_data)\n",
    "        # ######### CLEAR UNNAMED COLUMNS FROM DATASETS #######################################\n",
    "        self.recovered_data = self.recovered_data.loc[:, ~self.recovered_data.columns.str.contains('^Unnamed')]\n",
    "        return self.recovered_data\n",
    "\n",
    "    def _save_imputed(self, imputed_file=f'imputed_output.csv'):\n",
    "        self.imputed_file = imputed_file\n",
    "        print(imputed_file)\n",
    "        self.imputed_data.to_csv(self.imputed_file)\n",
    "\n",
    "    def _read_imputed(self, imputed_file=f'imputed_output.csv'):\n",
    "        self.imputed_file = imputed_file\n",
    "        print(imputed_file)\n",
    "        self.imputed_data = pd.read_csv(self.imputed_file)\n",
    "        # display(self.imputed_data)\n",
    "        # ######### CLEAR UNNAMED COLUMNS FROM DATASETS #######################################\n",
    "        self.imputed_data = self.recovered_data.loc[:, ~self.recovered_data.columns.str.contains('^Unnamed')]\n",
    "        return self.imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6340,
     "status": "ok",
     "timestamp": 1698374090824,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "XtgFFiGF0hD7",
    "outputId": "88ed53ad-c5e5-4cad-aee3-549565495720"
   },
   "outputs": [],
   "source": [
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "\n",
    "print(f'NumPy version:{np.__version__}')\n",
    "np.float = float # 'float32' # float\n",
    "table_dtype = np.float  #'float32'\n",
    "print(table_dtype)\n",
    "\n",
    "# hls_data = HLSDataSet(table_dtype = table_dtype)\n",
    "hls_data = HLSInference(table_dtype = table_dtype)\n",
    "\n",
    "hls_data._REFLECTANCE(round=3)\n",
    "\n",
    "hls_data.clip_dataset(x1=50.0, y1=50.0, x2=100.0, y2=100.0)\n",
    "\n",
    "doys = [171, 179, 187, 195, 203, 211, 219]\n",
    "doys = [171, 179, 187, 203, 211, 219]\n",
    "doys = [203, 211, 219]\n",
    "df = hls_data._get_data_doys(doys = doys, SHOW=True)\n",
    "display(df)\n",
    "\n",
    "# df = hls_data._set_columns_name()\n",
    "# display(df)\n",
    "\n",
    "df1, df2 = hls_data._nan_9999()\n",
    "# display(df1)\n",
    "# display(df2)\n",
    "\n",
    "df1, df2 = hls_data._set_clear_cloud()\n",
    "# display(df1)\n",
    "# display(df2)\n",
    "\n",
    "data, nan, clear, cloud = hls_data._set_train_columns_name()\n",
    "\n",
    "print('clear')\n",
    "display(clear)\n",
    "\n",
    "# train_data, test_data = hls_data._set_train_test_data(doy=211.0, x1=60.0, y1=40.0, x2=75.0, y2=55.0)\n",
    "\n",
    "train_data, test_data = hls_data._set_train_test_data(doy=211.0, x1=45.0, y1=45.0, x2=50.0, y2=50.0, for_show_nan=False)\n",
    "# train_data, test_data = hls_data._set_timeseries_train_test_data(doy=211.0, x1=50.0, y1=50.0, x2=100.0, y2=100.0)\n",
    "\n",
    "print('train_data', train_data['DOY'].unique())\n",
    "display(train_data)\n",
    "print('test_data')\n",
    "display(test_data)\n",
    "# display(data)\n",
    "# display(nan)\n",
    "# display(clear)\n",
    "# display(cloud)\n",
    "\n",
    "to_impute = hls_data._to_impute()\n",
    "hls_data._inference_train_test_data()\n",
    "hls_data._inference_imshow()\n",
    "\n",
    "# fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1698374090824,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "ZowJTFGIHI-b"
   },
   "outputs": [],
   "source": [
    "def GetModel(train_data, experiment_dir = '', load_model_path = '', tokenizer='',):\n",
    "\n",
    "    data = train_data.copy()\n",
    "    # Get the DataFrame with columns in reverse order\n",
    "    # data = data[data.columns[::-1]].copy()\n",
    "\n",
    "    print(data.dtypes.tolist())\n",
    "    # Reset the index to remove it\n",
    "    data = data.reset_index(drop=True)\n",
    "    display(data)\n",
    "\n",
    "    # Get the current CPU time in seconds since the epoch\n",
    "    current_time = int(time.time())\n",
    "    # Use the current time as a seed for a random number generator\n",
    "    random_seed_state = current_time  # You can use this random_state for various random processes\n",
    "\n",
    "    step_checkpoint = 12000\n",
    "    #### TRAINER HYPERPARAMETERS #############################\n",
    "    save_steps = 1000\n",
    "    logging_steps = 1\n",
    "    \n",
    "    epochs = 1\n",
    "    batch_size = 1\n",
    "    \n",
    "    learning_rate = 1e-12\n",
    "    lr_scheduler_type = 'constant'\n",
    "    # lr_scheduler_type = 'cosine'\n",
    "    num_cycles = 4\n",
    "    \n",
    "    warmup_steps = 5000\n",
    "    \n",
    "    optimizer = 'sophia'\n",
    "    ##########################################################\n",
    "\n",
    "    efficient_finetuning = '' #'lora'\n",
    "\n",
    "    print('experiment_dir :', experiment_dir)\n",
    "    print('load_model_path:', load_model_path)\n",
    "\n",
    "    # load_model_path = 'load_model/checkpoint-16000'\n",
    "    model = GReaT(llm=load_model_path,\n",
    "                  # tokenizer=load_model_path,\n",
    "                  tokenizer=tokenizer,\n",
    "                  batch_size=batch_size, epochs=epochs, max_steps=1,\n",
    "                  logging_steps=logging_steps, save_steps=save_steps,\n",
    "                  # evaluation_strategy='steps',\n",
    "                  # dataloader_num_workers=2, #fp16=True,\n",
    "                  logging_first_step=True,\n",
    "                  save_total_limit=2,\n",
    "                  prediction_loss_only=True,\n",
    "                  experiment_dir=experiment_dir,\n",
    "                  dataloader_num_workers=2,\n",
    "                  efficient_finetuning = efficient_finetuning,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lr_scheduler_type=lr_scheduler_type,\n",
    "                  warmup_steps = warmup_steps,\n",
    "                  num_cycles = num_cycles,\n",
    "                  # warmup_ratio=0.05,\n",
    "                  seed=current_time,\n",
    "                  data_seed=current_time+int(time.time()),\n",
    "                  # optim=TRAINER_DICT['optimizer'],\n",
    "                  fp16 = True,\n",
    "                  # torch_compile=True,   #### uncomment for Ampere\n",
    "                  # bf16=True,            #### uncomment for Ampere\n",
    "                  report_to='none',\n",
    "                  )\n",
    "\n",
    "    # model.load_from_dir(f'{load_model_path}')\n",
    "    # print(f'----------- Model architecture, efficient_finetuning: {efficient_finetuning} -----------------------')\n",
    "    # print(model.model)\n",
    "    # print(f'----------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    model.fit(data=data[0:2], test_data=data[0:2], resume_from_checkpoint=True)\n",
    "    # model.fit(data[0:1])\n",
    "\n",
    "    return model\n",
    "\n",
    "# fn\n",
    "# model.fit(data=train_data, test_data=test_data)\n",
    "\n",
    "# TRAINER_DICT.to_csv(f'{experiment_dir}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 690889,
     "status": "ok",
     "timestamp": 1698374781701,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "3WhEgLr31fmg",
    "outputId": "48086a27-6ad5-4907-ecbb-857d9def648c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pwd\n",
    "\n",
    "\n",
    "# i_run = 0\n",
    "\n",
    "EXP_NAME = 'A0[RGB_TOK[ai60]]'\n",
    "# EXP_NAME = f'A0[RGB_TOK[ai]_Lora]'\n",
    "EXP_NAME = f'A0[RGBXY_TOK[ai]]'\n",
    "EXP_NAME = f'A0[RGB_PID[int]]'\n",
    "EXP_NAME = f'A0[RGB_PID[int]_season]'\n",
    "EXP_NAME = f'A0[RGB_REFL[float]]'\n",
    "\n",
    "\n",
    "#### LISTS for TRAINER_RUN cycles POLYNOMIAL WITH WARMUP ###################################\n",
    "epochs_list = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "max_steps_list =    [150000, 108000, 10000, 20000, 20000, 20000, 20000, 20000]\n",
    "warmup_steps_list = [15000,  5000,  5000,      0,     0,     0,     0,     0]\n",
    "\n",
    "lr_scheduler_type_list = ['constant', 'constant', 'constant', 'polynomial', 'polynomial', 'polynomial', 'polynomial', 'polynomial' ]     # 'polynomial'      \n",
    "num_cycles = 3\n",
    "\n",
    "learning_rate_list = [1e-7, 1e-6, 1e-4, 1e-4, 7e-5, 2e-5, 7e-6, 2e-6]\n",
    "########################################################################\n",
    "step_checkpoint_list = max_steps_list   # [16000, 175000, 175000, 175000, 175000]\n",
    "\n",
    "\n",
    "\n",
    "load_model_path_list = []\n",
    "experiment_dir_list  = []\n",
    "\n",
    "for i_run, chkpt in enumerate(step_checkpoint_list):\n",
    "    experiment_dir_list.append(f'{EXP_NAME}/run[{i_run}]')\n",
    "    load_model_path_list.append(f'{EXP_NAME}/run[{i_run}]/checkpoint-{chkpt}')\n",
    "    # load_model_path_list.append(f'run[{i_run}]/model')\n",
    "\n",
    "i_run = 0\n",
    "\n",
    "print(load_model_path_list)\n",
    "\n",
    "print('i_run:', i_run, load_model_path_list[i_run])\n",
    "\n",
    "train_columns_list = ['B02', 'B03', 'B04', 'PID', 'DOY']\n",
    "\n",
    "# # Create a new column 'BAND' by concatenating 'B02', 'B03', and 'B04' with ';'\n",
    "# train_data['BAND'] = train_data.apply(lambda row: ';'.join([str(row['B02']), str(row['B03']), str(row['B04'])]), axis=1)\n",
    "# train_columns_list = ['PID', 'DOY', 'BAND']\n",
    "\n",
    "#### CODE DATA 4 TRAIN #####################################\n",
    "# train_columns_list = ['PID', 'DOY', 'BAND']\n",
    "train_columns_list = ['B02', 'B03', 'B04', 'PID', 'DOY']\n",
    "# train_columns_list = ['B02', 'B03', 'B04', 'X', 'Y', 'DOY']\n",
    "# train_columns_list = ['B03', 'PID', 'DOY']\n",
    "# data = train_data[train_columns_list].copy()\n",
    "\n",
    "# train_data['B02'] = train_data['B02'] * 1000\n",
    "# train_data['B03'] = train_data['B03'] * 1000\n",
    "# train_data['B04'] = train_data['B04'] * 1000\n",
    "# train_data = train_data.astype(int)\n",
    "################################\n",
    "\n",
    "# i_run = 0\n",
    "model = GetModel(train_data[train_columns_list], \n",
    "                 experiment_dir = experiment_dir_list[i_run], \n",
    "                 load_model_path = load_model_path_list[i_run], \n",
    "                 # tokenizer=load_model_path_list[i_run],)\n",
    "                 tokenizer=f\"{EXP_NAME}/aispace-tokenizer\")\n",
    "# model = []\n",
    "\n",
    "recovered_data = hls_data._impute(model=model, k=10000, max_length=50, temperature=1e-32) #, device='cpu')\n",
    "\n",
    "imputed_file = f'{EXP_NAME}/recovered_output_run[{i_run}].csv'\n",
    "hls_data._save_recovered(imputed_file=imputed_file)\n",
    "\n",
    "imputed_file = f'{EXP_NAME}/imputed_output_run[{i_run}].csv'\n",
    "hls_data._save_imputed(imputed_file=imputed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2391,
     "status": "ok",
     "timestamp": 1698374873218,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "52mz1GuEoAVH",
    "outputId": "d62911da-829e-4557-95c6-3617f24bb287",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "i_run=0\n",
    "\n",
    "# imputed_file = f'{experiment_dir}/imputed_output_run[{i_run}].csv'\n",
    "# imputed_data = hls_data._read_imputed(imputed_file)\n",
    "\n",
    "imputed_file = f'{EXP_NAME}/recovered_output_run[{i_run}].csv'\n",
    "recovered_data = hls_data._read_recovered(imputed_file)\n",
    "\n",
    "# display(imputed_data)\n",
    "print('recovered_data')\n",
    "display(recovered_data)\n",
    "\n",
    "hls_data._set_inference_recovered()\n",
    "hls_data._inference_imshow()\n",
    "\n",
    "\n",
    "# fn\n",
    "# experiment_dir = 'A0[shuf_sophia]'\n",
    "# experiment_dir = 'A0[great_sophia]'\n",
    "imputed_file = f'{EXP_NAME}/recovered_output_run[{i_run}].csv'\n",
    "for i_run in range(0,4):\n",
    "  imputed_file = f'{EXP_NAME}/recovered_output_run[{i_run}].csv'\n",
    "  recovered_data = hls_data._read_recovered(imputed_file)\n",
    "\n",
    "  hls_data._set_inference_recovered()\n",
    "  hls_data._inference_imshow()\n",
    "# display(recovered_data)\n",
    "# recovered_data = pd.read_csv(imputed_file)\n",
    "\n",
    "# display(recovered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1698374782154,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "V87t1ZruBY5w"
   },
   "outputs": [],
   "source": [
    "# hls_data._imputed_data(recovered_data)\n",
    "\n",
    "hls_data._set_inference_recovered()\n",
    "\n",
    "# hls_data._inference_train_test_data()\n",
    "hls_data._inference_imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1698374782154,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "0ZfvhzTy5RU3"
   },
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1698374782154,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "tQxA80SIzte8"
   },
   "outputs": [],
   "source": [
    "# #### TEST METRICS ##################\n",
    "# nan_data = test_data.copy()\n",
    "# nan_data = nan_data.reset_index(drop=True)\n",
    "\n",
    "# print(nan_data.dtypes.tolist())\n",
    "\n",
    "# display(nan_data)\n",
    "# print(nan_data.columns)\n",
    "# # fn\n",
    "\n",
    "# print(f'NumPy version:{np.__version__}')\n",
    "# np.float = float\n",
    "\n",
    "# imputed_data = model.impute(nan_data, k=256, max_length=50000, temperature=0.01) #, device='cpu')\n",
    "# imputed_data.to_csv('test_imputed_output.csv')\n",
    "\n",
    "# # imputed_data = pd.read_csv('imputed_output.csv')\n",
    "# # ######### CLEAR UNNAMED COLUMNS FROM DATASETS #######################################\n",
    "# # imputed_data = imputed_data.loc[:, ~imputed_data.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1698374782155,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "qQBC-ZXEHI-c"
   },
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1698374782155,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "AHzEWLYbB-IY"
   },
   "outputs": [],
   "source": [
    "data = clear.copy() #[0:1000]\n",
    "\n",
    "table_dtype = 'int16'\n",
    "\n",
    "data = data.astype(table_dtype)\n",
    "print(data.dtypes.tolist())\n",
    "# Reset the index to remove it\n",
    "data = data.reset_index(drop=True)\n",
    "# train_data.columns = final_columns_list\n",
    "# display(data)\n",
    "\n",
    "\n",
    "# Split the DataFrame into training and test DataFrames\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "display(train_data)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "display(test_data)\n",
    "\n",
    "# #### FOR CPU USABILITY, for 2 pixels #################################\n",
    "# epochs_steps = 1  # 250 #50\n",
    "# save_steps = 1   #250 #50 #9380  #2000   # FOR FIT THIS VALUE START TRAIN DATASET, EQUALS FOR STEPS IN FIRST START\n",
    "# logging_steps = 1 # 250 #25 #9380  #1000\n",
    "\n",
    "#### FOR T-4 GPU USABILITY################################\n",
    "epochs_steps = 200   # 250 #50\n",
    "save_steps = 100   #250 #50 #9380  #2000   # FOR FIT THIS VALUE START TRAIN DATASET, EQUALS FOR STEPS IN FIRST START\n",
    "logging_steps = 50 # 250 #25 #9380  #1000\n",
    "#### batch_size = 250 for T4\n",
    "batch_size = 200 #250 #312  # 224 # 250 #300 #164 for float64 # 112 # for float64 # 136 # 96   # 140# 400\n",
    "#######################################\n",
    "\n",
    "# #### FOR V-100 GPU USABILITY################################\n",
    "# epochs_steps = 200   # 250 #50\n",
    "# save_steps = 1000   #250 #50 #9380  #2000   # FOR FIT THIS VALUE START TRAIN DATASET, EQUALS FOR STEPS IN FIRST START\n",
    "# logging_steps = 200 # 250 #25 #9380  #1000\n",
    "# #### batch_size = 250 for T4\n",
    "# batch_size = 232 #250 #312  # 224 # 250 #300 #164 for float64 # 112 # for float64 # 136 # 96   # 140# 400\n",
    "# #######################################\n",
    "\n",
    "# #### FOR A-100 GPU USABILITY################################\n",
    "# epochs_steps = 800   # 250 #50\n",
    "# save_steps = 10000   #250 #50 #9380  #2000   # FOR FIT THIS VALUE START TRAIN DATASET, EQUALS FOR STEPS IN FIRST START\n",
    "# logging_steps = 500 # 250 #25 #9380  #1000\n",
    "# #### batch_size = 250 for T4\n",
    "# batch_size = 800 #250 #312  # 224 # 250 #300 #164 for float64 # 112 # for float64 # 136 # 96   # 140# 400\n",
    "# #######################################\n",
    "\n",
    "EXP_NAME = 'exp-A100'\n",
    "EXP_NAME = 'exp-V100'\n",
    "EXP_NAME = 'exp-T4'\n",
    "\n",
    "EXP_NAME = 'A0BASE'\n",
    "\n",
    "#### 1 ###########\n",
    "# learning_rate = 5e-5\n",
    "# lr_scheduler_type = 'constant' # constant_with_warmup\n",
    "\n",
    "step_checkpoint = 16000\n",
    "######### SET TRAINER_RUN ARGUMENTS ##########################\n",
    "learning_rate = 0.00001\n",
    "lr_scheduler_type = 'cosine_with_restarts'\n",
    "lr_scheduler_type = 'constant_with_warmup'\n",
    "lr_scheduler_type = 'cosine'\n",
    "lr_scheduler_type = 'linear'\n",
    "\n",
    "TRAINER_RUN = 0\n",
    "if TRAINER_RUN == 0:\n",
    "  load_model_path = 'distilgpt2'\n",
    "  experiment_dir = f\"{EXP_NAME}/run[{TRAINER_RUN}]\"\n",
    "elif TRAINER_RUN > 0:\n",
    "  before = pd.read_csv(f'{EXP_NAME}/run[{TRAINER_RUN-1}].csv')\n",
    "  load_model_path = before['experiment_dir'][0] + f'/checkpoint-{step_checkpoint}'\n",
    "  experiment_dir  = f\"{EXP_NAME}/run[{TRAINER_RUN}]\"\n",
    "\n",
    "TRAINER_DICT = pd.DataFrame({'EXP_NAME' : EXP_NAME}, index = [TRAINER_RUN])    # dict()\n",
    "\n",
    "\n",
    "TRAINER_DICT['TRAINER_RUN'] = TRAINER_RUN\n",
    "TRAINER_DICT['table_dtype'] = table_dtype\n",
    "TRAINER_DICT['epochs_steps'] = epochs_steps\n",
    "TRAINER_DICT['save_steps'] = save_steps\n",
    "TRAINER_DICT['logging_steps'] = logging_steps\n",
    "TRAINER_DICT['batch_size'] = batch_size\n",
    "TRAINER_DICT['learning_rate'] = learning_rate\n",
    "TRAINER_DICT['lr_scheduler_type'] = lr_scheduler_type\n",
    "TRAINER_DICT['experiment_dir'] = experiment_dir\n",
    "\n",
    "display(pd.DataFrame.from_dict(TRAINER_DICT))\n",
    "\n",
    "##### SET experiment_dir & efficient_finetuning #######################\n",
    "efficient_finetuning = ''  #'lora'\n",
    "if efficient_finetuning == 'lora':\n",
    "    experiment_dir = f\"{experiment_dir}_{efficient_finetuning}\"\n",
    "##### SET experiment_dir & efficient_finetuning #######################\n",
    "optimizer = 'adamw_torch'\n",
    "# optimizer = 'adamw_torch_fused'  #'adamw_torch'  #'adamw_torch_fused'\n",
    "if optimizer == 'adamw_torch_fused':\n",
    "    experiment_dir = f\"{experiment_dir}_{optimizer}\"\n",
    "#### SET model_save_dir through save_model #########################\n",
    "# model_save_dir = f'{EXP_NAME}/ZeroModel_{efficient_finetuning}'\n",
    "\n",
    "\n",
    "print('experiment_dir :', experiment_dir)\n",
    "print('load_model_path:', load_model_path)\n",
    "\n",
    "\n",
    "model = GReaT(llm=load_model_path,\n",
    "              batch_size=batch_size, epochs=epochs_steps, logging_steps=logging_steps, save_steps=save_steps,\n",
    "              # evaluation_strategy='steps',\n",
    "              # dataloader_num_workers=2, #fp16=True,\n",
    "              save_total_limit=2,\n",
    "              prediction_loss_only=True,\n",
    "              experiment_dir=f'{experiment_dir}',\n",
    "              dataloader_num_workers=2,\n",
    "              efficient_finetuning = efficient_finetuning,\n",
    "              learning_rate=learning_rate,\n",
    "              lr_scheduler_type=lr_scheduler_type,\n",
    "              warmup_ratio=0.1,\n",
    "              # optim=optimizer,\n",
    "              fp16 = True,\n",
    "              # torch_compile=True,   #### for Ampere\n",
    "              # bf16=True,            #### for Ampere\n",
    "              )\n",
    "\n",
    "print(f'----------- Model architecture, efficient_finetuning: {efficient_finetuning} -----------------------')\n",
    "print(model.model)\n",
    "print(f'----------------------------------------------------------------------------------------------------')\n",
    "# fn\n",
    "\n",
    "model.fit(data=train_data)\n",
    "# model.fit(data=train_data, test_data=test_data)\n",
    "\n",
    "TRAINER_DICT.to_csv(f'{experiment_dir}.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1698374782155,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "LV037qqBNdol"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1698374782155,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "695cBCyoBG3q"
   },
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "aborted",
     "timestamp": 1698374782155,
     "user": {
      "displayName": "Koyaua Uvarov",
      "userId": "04551130653342672393"
     },
     "user_tz": -180
    },
    "id": "f9v8lVF5Sayy"
   },
   "outputs": [],
   "source": [
    "fn"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
