# Virtual Satellite

GNU GENERAL PUBLIC LICENSE
Version 3, 29 June 2007
Copyright (C) 2023 Nikolay Uvarov

This project uses the GPT-2 model, which is licensed under the MIT License.
© OpenAI (https://github.com/openai/gpt-2)
Additional modifications © 2025 Nikolay Uvarov

This project was inspired by the research presented in https://arxiv.org/abs/2210.06280 and https://arxiv.org/abs/2403.07815 and https://www.nature.com/articles/s42256-023-00639-z.

The primary objective of this study is to assess the feasibility of applying Large Language Models for addressing challenges in satellite imagery (for regression tasks), including tasks such as: generating Cloud-Free Satellite Images, creating continuous Virtual Satellite imagery, and more.

This project utilized the LLM Distilled GPT-2 version from [Hugging Face](https://huggingface.co/distilgpt2) because it more faster than the base GPT-2 and more suitable for fast R&D purposes. Additionally, [Sophia's](https://arxiv.org/abs/2305.14342) [ 2nd order optimizer](https://github.com/kyegomez/Sophia) was implemented for Hugging Face Trainer for rapid training and cost reduction. Originally GPT-2 developed by OpenAI and distributed under the MIT License.

The 'data' folder contains Landsat-8 HLS (Harmonized Landsat and Sentinel-2) time series images.

- A0.py, A0.sh, or ai.ipynb file contains training flows for experimenting with GPT-2 fine-tuning (schedulers, etc.).
- ainf.ipynb for inferencing.
******************************************************************************************

Example of Landsat-8 harmonized time series images, original and masked images. The masked Area of Interest (AOI) is excluded from the training dataset and will be generated by GPT-2 after the training (fine-tuning) process.

<img width="657" alt="Landsat-8" src="https://github.com/koyacolab/aispace/assets/115004547/dcc1853c-8655-4b5d-ab28-0b10dd50fd2c">

******************************************************************************************

Results, 

50x50 Landsat-8 patch:

<img width="632" alt="50x50" src="https://github.com/koyacolab/aispace/assets/115004547/fb1f6389-effe-43b2-9914-c042217d1826">

*****************************************************************************************

100x100 Landsat-8 patch:

<img width="632" alt="100x100" src="https://github.com/koyacolab/aispace/assets/115004547/01f2e598-9c25-478d-b50c-7d2f3382b073">

*****************************************************************************************

100x100 Landsat-8 patch (with numerical tokenizer+embedings, point II from next steps):

<img width="632" alt="Screenshot 2024-02-11 at 18 46 10" src="https://github.com/koyacolab/aispace/assets/115004547/25df22e0-fc76-49c9-94af-c71aeb7d316b">

*****************************************************************************************

Conclusions and next steps:

I. (In progress) Autoregressive Language Models such as GPT-2 utilize causals from the left to the right. Autoregressive approaches are preferable for generating long sequences (for example, entire documents), but since such causal models only condition on previous tokens, they are worse applicable to be applied to text-infilling tasks and not profitable from MLM pre-training. In this project, this requires random permutations of tokens, which leads to increased training time as the dataset size increases. However, MLM approaches fail to generate longer sequences due to their independence assumption. To unify both worlds and retain the benefits of autoregressive modeling in combination with a bidirectional context, several methods will be provided in the next steps.

II. (Done) Representing every number as a single token is suboptimal due to a lack of generalization to new numbers and the sparsity of the provided tokens. Due to the inherent structure of numbers, learning the embeddings of numerical tokens in a purely data-driven way is ineffective. Moreover, since the GPT-2 is trained with cross-entropy loss, no notion of similarity between numerical tokens is conveyed. As a remedy, the simple inductive bias about the semantic proximity of numerical tokens, similar to positional encodings will be provided in the next steps. 

III. Today this POC project utilizes only optical data from the Landsat-8 satellite, the imagery frequency of which suffers due to the cloudiness and the sparse revisit time. In the future, this will be supplemented by Sentinel-1 Synthetic Aperture Radar images.

GNU GENERAL PUBLIC LICENSE
Version 3, 29 June 2007
Copyright (C) 2023 Nikolay Uvarov
